{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# metricas\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# gestion de train-test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# transformaciones\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample ID</th>\n",
       "      <th>Grab ID</th>\n",
       "      <th>Profile ID</th>\n",
       "      <th>Sample Number</th>\n",
       "      <th>Collect DateTime</th>\n",
       "      <th>Depth (m)</th>\n",
       "      <th>Site Type</th>\n",
       "      <th>Area</th>\n",
       "      <th>Locator</th>\n",
       "      <th>Site</th>\n",
       "      <th>...</th>\n",
       "      <th>MDL</th>\n",
       "      <th>RDL</th>\n",
       "      <th>Text Value</th>\n",
       "      <th>Sample Info</th>\n",
       "      <th>Steward Note</th>\n",
       "      <th>Replicates</th>\n",
       "      <th>Replicate Of</th>\n",
       "      <th>Method</th>\n",
       "      <th>Date Analyzed</th>\n",
       "      <th>Data Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16316</td>\n",
       "      <td>16316.0</td>\n",
       "      <td>10702</td>\n",
       "      <td>9209019</td>\n",
       "      <td>04/13/1992 12:00:00 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Streams and Rivers</td>\n",
       "      <td>Pipers</td>\n",
       "      <td>KSHZ06</td>\n",
       "      <td>Pipers Creek mouth</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.070||King County Nstream Database/B53311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KCEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8937</td>\n",
       "      <td>8937.0</td>\n",
       "      <td>37688</td>\n",
       "      <td>7915489</td>\n",
       "      <td>06/20/1979 12:00:00 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Streams and Rivers</td>\n",
       "      <td>Crisp</td>\n",
       "      <td>0321</td>\n",
       "      <td>Crisp Creek mouth at SE Green Valley Rd</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.727||King County Nstream Database/RS2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KCEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137745</td>\n",
       "      <td>137745.0</td>\n",
       "      <td>54368</td>\n",
       "      <td>L58228-1</td>\n",
       "      <td>06/25/2013 08:09:00 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Large Lakes</td>\n",
       "      <td>Lake Union/Ship Canal</td>\n",
       "      <td>0512</td>\n",
       "      <td>Ship Canal above locks</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HYDROLAB</td>\n",
       "      <td>06/25/2013</td>\n",
       "      <td>KCEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131816</td>\n",
       "      <td>131816.0</td>\n",
       "      <td>50605</td>\n",
       "      <td>L55068-6</td>\n",
       "      <td>02/13/2012 09:38:00 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Large Lakes</td>\n",
       "      <td>Lake Union/Ship Canal</td>\n",
       "      <td>0540</td>\n",
       "      <td>Ship Canal near Montlake Bridge</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SM4500-P-F</td>\n",
       "      <td>02/15/2012</td>\n",
       "      <td>KCEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82325</td>\n",
       "      <td>82325.0</td>\n",
       "      <td>43896</td>\n",
       "      <td>L52933-87</td>\n",
       "      <td>03/30/2011 02:36:00 PM</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Large Lakes</td>\n",
       "      <td>Lake Washington</td>\n",
       "      <td>0804</td>\n",
       "      <td>Lake Washington north end</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HYDROLAB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KCEL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample ID   Grab ID  Profile ID Sample Number        Collect DateTime  \\\n",
       "0      16316   16316.0       10702       9209019  04/13/1992 12:00:00 AM   \n",
       "1       8937    8937.0       37688       7915489  06/20/1979 12:00:00 AM   \n",
       "2     137745  137745.0       54368      L58228-1  06/25/2013 08:09:00 AM   \n",
       "3     131816  131816.0       50605      L55068-6  02/13/2012 09:38:00 AM   \n",
       "4      82325   82325.0       43896     L52933-87  03/30/2011 02:36:00 PM   \n",
       "\n",
       "   Depth (m)           Site Type                   Area Locator  \\\n",
       "0        1.0  Streams and Rivers                 Pipers  KSHZ06   \n",
       "1        1.0  Streams and Rivers                  Crisp    0321   \n",
       "2        1.0         Large Lakes  Lake Union/Ship Canal    0512   \n",
       "3        1.0         Large Lakes  Lake Union/Ship Canal    0540   \n",
       "4        4.2         Large Lakes        Lake Washington    0804   \n",
       "\n",
       "                                      Site  ...    MDL    RDL  \\\n",
       "0                       Pipers Creek mouth  ...    NaN    NaN   \n",
       "1  Crisp Creek mouth at SE Green Valley Rd  ...    NaN    NaN   \n",
       "2                   Ship Canal above locks  ...    NaN    NaN   \n",
       "3          Ship Canal near Montlake Bridge  ...  0.002  0.005   \n",
       "4                Lake Washington north end  ...    NaN    NaN   \n",
       "\n",
       "                                  Text Value  Sample Info Steward Note  \\\n",
       "0  .070||King County Nstream Database/B53311          NaN          NaN   \n",
       "1     .727||King County Nstream Database/RS2          NaN          NaN   \n",
       "2                                        NaN          NaN          NaN   \n",
       "3                                        NaN          NaN          NaN   \n",
       "4                                        NaN          NaN          NaN   \n",
       "\n",
       "   Replicates  Replicate Of      Method Date Analyzed Data Source  \n",
       "0         NaN           NaN        none           NaN        KCEL  \n",
       "1         NaN           NaN         NaN           NaN        KCEL  \n",
       "2         NaN           NaN    HYDROLAB    06/25/2013        KCEL  \n",
       "3         NaN           NaN  SM4500-P-F    02/15/2012        KCEL  \n",
       "4         NaN           NaN    HYDROLAB           NaN        KCEL  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0 = pd.read_csv(\"SRC/DATA/water-quality-1.csv\")\n",
    "data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 25)\n"
     ]
    }
   ],
   "source": [
    "data = data0[(data0['Site Type'] == \"Large Lakes\") | (data0['Site Type'] == 'Streams and Rivers')]\n",
    "dataPro = data.sample(30000, random_state=2024)\n",
    "print(dataPro.shape)\n",
    "#dataPro = data.sample(n=10000, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 5)) \n",
    "dataPro['Site Type'].hist(bins=20, density=True, color='red', alpha=0.3)\n",
    "plt.title('Site Type') \n",
    "  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se analizo que la variable Data Source solo tiene un valor, por lo que no aportara nada al modelo y se decide eliminarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPro.drop(\"Data Source\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imputación de Datos Faltantes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificamos porcentaje faltantes por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample ID           0.000000\n",
       "Grab ID             0.282233\n",
       "Profile ID          0.000000\n",
       "Sample Number       0.000000\n",
       "Collect DateTime    0.000000\n",
       "Depth (m)           0.282233\n",
       "Site Type           0.000000\n",
       "Area                0.000000\n",
       "Locator             0.000000\n",
       "Site                0.000000\n",
       "Parameter           0.000000\n",
       "Value               0.089633\n",
       "Units               0.000567\n",
       "QualityId           0.000000\n",
       "Lab Qualifier       0.879100\n",
       "MDL                 0.517233\n",
       "RDL                 0.517533\n",
       "Text Value          0.815900\n",
       "Sample Info         1.000000\n",
       "Steward Note        0.999867\n",
       "Replicates          0.998467\n",
       "Replicate Of        0.998767\n",
       "Method              0.156933\n",
       "Date Analyzed       0.564533\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPro.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Escala de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample Number',\n",
       " 'Collect DateTime',\n",
       " 'Site Type',\n",
       " 'Area',\n",
       " 'Locator',\n",
       " 'Site',\n",
       " 'Parameter',\n",
       " 'Units',\n",
       " 'Lab Qualifier',\n",
       " 'Text Value',\n",
       " 'Sample Info',\n",
       " 'Steward Note',\n",
       " 'Method',\n",
       " 'Date Analyzed']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricas = [col for col in dataPro.columns if(dataPro[col].dtypes == 'object')]\n",
    "categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample ID',\n",
       " 'Grab ID',\n",
       " 'Profile ID',\n",
       " 'Depth (m)',\n",
       " 'Value',\n",
       " 'MDL',\n",
       " 'RDL',\n",
       " 'Replicates',\n",
       " 'Replicate Of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuas = [col for col in dataPro.columns if((dataPro[col].dtypes in ['int64', 'float64']) and len(dataPro[col].unique()) > 30)]\n",
    "continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QualityId']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretas = [col for col in dataPro.columns if((dataPro[col].dtypes in ['int64', 'float64']) and len(dataPro[col].unique()) <= 30)]\n",
    "discretas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. - Balanceo del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Site Type\n",
       "Large Lakes           0.639433\n",
       "Streams and Rivers    0.360567\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPro['Site Type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Site Type')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "dataPro['Site Type'].hist(bins=20, density=True, color='red', alpha=0.3)\n",
    "plt.title('Site Type')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seleccion de Target y Features\n",
    "X = dataPro.drop(\"Site Type\", axis=1) #Feature - lo que voy a usar para predecir\n",
    "y = dataPro[\"Site Type\"] # target - lo que voy a predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.- Imputacion de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grab ID         0.282233\n",
       "Depth (m)       0.282233\n",
       "Value           0.089633\n",
       "MDL             0.517233\n",
       "RDL             0.517533\n",
       "Replicates      0.998467\n",
       "Replicate Of    0.998767\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuas_con_na = [col for col in continuas if dataPro[col].isnull().mean() > 0]\n",
    "dataPro[continuas_con_na].isnull().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDL             0.517233\n",
       "RDL             0.517533\n",
       "Replicates      0.998467\n",
       "Replicate Of    0.998767\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuas_con_na = [col for col in continuas_con_na if dataPro[col].isnull().mean() > 0.5]\n",
    "dataPro[continuas_con_na].isnull().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPro.drop(continuas_con_na, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample ID     0.000000\n",
       "Grab ID       0.282233\n",
       "Profile ID    0.000000\n",
       "Depth (m)     0.282233\n",
       "Value         0.089633\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuas = [col for col in dataPro.columns if((dataPro[col].dtypes in ['int64', 'float64']) and len(dataPro[col].unique()) > 30)]\n",
    "dataPro[continuas].isnull().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grab ID      0.282233\n",
       "Depth (m)    0.282233\n",
       "Value        0.089633\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "continuas_con_na = [col for col in continuas if dataPro[col].isnull().mean() > 0]\n",
    "dataPro[continuas_con_na].isnull().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample ID     0.000000\n",
       "Grab ID       0.282233\n",
       "Profile ID    0.000000\n",
       "Depth (m)     0.282233\n",
       "Value         0.089633\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuas = [col for col in dataPro.columns if((dataPro[col].dtypes in ['int64', 'float64']) and len(dataPro[col].unique()) > 30)]\n",
    "dataPro[continuas].isnull().mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_GrabID = round(dataPro['Grab ID'].mean(), 2)\n",
    "dataPro['Grab ID'].fillna(mean_GrabID, inplace=True)\n",
    "\n",
    "median_Depth = round(dataPro['Depth (m)'].median(), 2)\n",
    "dataPro['Depth (m)'].fillna(median_Depth, inplace=True)\n",
    "\n",
    "mean_value = round(dataPro['Value'].mean(), 2)\n",
    "dataPro['Value'].fillna(mean_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas_con_na = [col for col in categoricas if dataPro[col].isnull().mean() > 0]\n",
    "dataPro[categoricas_con_na].isnull().mean() \n",
    "\n",
    "categoricas_con_na_eliminar = [col for col in categoricas if dataPro[col].isnull().mean() > 0.2]\n",
    "dataPro[categoricas_con_na_eliminar].isnull().mean() \n",
    "\n",
    "dataPro.drop(categoricas_con_na_eliminar, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample ID           0.0\n",
       "Grab ID             0.0\n",
       "Profile ID          0.0\n",
       "Sample Number       0.0\n",
       "Collect DateTime    0.0\n",
       "Depth (m)           0.0\n",
       "Site Type           0.0\n",
       "Area                0.0\n",
       "Locator             0.0\n",
       "Site                0.0\n",
       "Parameter           0.0\n",
       "Value               0.0\n",
       "Units               0.0\n",
       "QualityId           0.0\n",
       "Method              0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricas = [col for col in dataPro.columns if(dataPro[col].dtypes == 'object')]\n",
    "categoricas\n",
    "\n",
    "categoricas_con_na = [col for col in categoricas if dataPro[col].isnull().mean() > 0]\n",
    "dataPro[categoricas_con_na].isnull().mean() \n",
    "\n",
    "dataPro['Area'].fillna('Lake Washington', inplace=True)\n",
    "\n",
    "dataPro['Units'].fillna('mg/L', inplace=True)\n",
    "\n",
    "dataPro['Method'].fillna('NONE', inplace=True)\n",
    "\n",
    "dataPro.isnull().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.- Codificacion de Variables Categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para todas las variables excepto  **Site Type** aplicamos una codificacion por frecuencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para todas las variables excepto **Site Type** aplicamos una codificacion por frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colName in categoricas:\n",
    "    if(colName not in [\"Site Type\"]):\n",
    "        encoder_dict = dataPro[colName].value_counts().to_dict()\n",
    "        dataPro[colName]=dataPro[colName].map(encoder_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2- Codificacion de Variable Target (Site Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "dataPro[\"Site Type\"] = encoder.fit_transform(dataPro[\"Site Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Tratamiento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedimiento para tratamiento de outliers con 1.5*IQR\n",
    "\n",
    "for colName in continuas:\n",
    "    IQR = dataPro[colName].quantile(0.75)-dataPro[colName].quantile(0.25)\n",
    "    LI = dataPro[colName].quantile(0.25)-1.5*IQR\n",
    "    LS = dataPro[colName].quantile(0.75)+1.5*IQR\n",
    "\n",
    "    dataPro[colName] = np.where(dataPro[colName]>LS,LS, \n",
    "                                    np.where(dataPro[colName]<LI,LI,dataPro[colName]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.- Contruccion de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seleccion de Target y Features\n",
    "X = dataPro.drop(\"Site Type\", axis=1) #Feature - lo que voy a usar para predecir\n",
    "y = dataPro[\"Site Type\"] # target - lo que voy a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split para entrenamiento y prueba.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos y aplicamos escalado de datos.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train) #calculamos el scaler\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9996889897916232\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train_scaled,y_train)\n",
    "\n",
    "nb_predicts = nb_classifier.predict(X_test_scaled) \n",
    "acc = roc_auc_score(nb_predicts,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 - Naive Bayes - Hyperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9996889897916232\n"
     ]
    }
   ],
   "source": [
    "nb_classifier1 = GaussianNB(priors=[0.4, 0.6],var_smoothing=1e-11 )\n",
    "\n",
    "nb_classifier1.fit(X_train_scaled,y_train)\n",
    "\n",
    "nb_predicts1 = nb_classifier1.predict(X_test_scaled) \n",
    "acc = roc_auc_score(nb_predicts1,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9996889897916232\n"
     ]
    }
   ],
   "source": [
    "nb_classifier2 = GaussianNB(priors=[0.3, 0.7],var_smoothing=1e-7 )\n",
    "\n",
    "nb_classifier2.fit(X_train_scaled,y_train)\n",
    "\n",
    "nb_predicts2 = nb_classifier2.predict(X_test_scaled) \n",
    "acc = roc_auc_score(nb_predicts2,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9996889897916232\n"
     ]
    }
   ],
   "source": [
    "nb_classifier3 = GaussianNB(var_smoothing=1e-7 )\n",
    "\n",
    "nb_classifier3.fit(X_train_scaled,y_train)\n",
    "\n",
    "nb_predicts3 = nb_classifier3.predict(X_test_scaled) \n",
    "acc = roc_auc_score(nb_predicts3,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9996889897916232\n"
     ]
    }
   ],
   "source": [
    "nb_classifier4 = GaussianNB(priors=[0.3, 0.7] )\n",
    "\n",
    "nb_classifier4.fit(X_train_scaled,y_train)\n",
    "\n",
    "nb_predicts4 = nb_classifier4.predict(X_test_scaled) \n",
    "acc = roc_auc_score(nb_predicts4,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'priors': None, 'var_smoothing': 1e-09}\n",
      "Best score achieved:  1.0\n",
      "NB - Roc Auc Score:  0.9996889897916232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "60 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'priors' parameter of GaussianNB must be an array-like or None. Got 0.3 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'priors' parameter of GaussianNB must be an array-like or None. Got 0.7 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan  1.  1.  1.  1.  1.  1.]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# NB Best.fit\n",
    "hyper_parameters = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4],\n",
    "    'priors':[0.3, 0.7,None]\n",
    "}\n",
    "\n",
    "# Inicializar el clasificador\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Configuración de la búsqueda de hiperparámetros\n",
    "nb_hyp_opt = GridSearchCV(estimator=nb_classifier, param_grid=hyper_parameters, cv=5, scoring=\"roc_auc\")\n",
    "\n",
    "# Entrenar el modelo y buscar los mejores hiperparámetros\n",
    "nb_hyp_opt.fit(X, y)\n",
    "\n",
    "# Revisar los mejores hiperparámetros y el mejor modelo\n",
    "print(\"Best parameters found: \", nb_hyp_opt.best_params_)\n",
    "print(\"Best score achieved: \", nb_hyp_opt.best_score_)\n",
    "\n",
    "acc = roc_auc_score(nb_predicts4,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - LDA   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases predichas: [1]\n",
      "LDA - ROC AUC Score: 0.4543791588195731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LDA \n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "lda.fit(X_train, y_train)\n",
    "  \n",
    "# Verificar las clases predichas\n",
    "y_pred = lda.predict(X_test_scaled)\n",
    "print(\"Clases predichas:\", np.unique(y_pred))\n",
    "\n",
    "# Asegurarse de que haya más de una clase en y_test\n",
    "if len(np.unique(y_test)) < 2:\n",
    "    raise ValueError(\"y_test debe contener al menos dos clases para calcular ROC AUC.\")\n",
    "\n",
    "# Calcular el ROC AUC\n",
    "y_score = lda.predict_proba(X_test_scaled)[:, 1]\n",
    "acc = roc_auc_score(y_test, y_score)\n",
    "print(\"LDA - ROC AUC Score:\", acc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases predichas: [1]\n",
      "LDA - ROC AUC Score: 0.4533277140300528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LDA 1\n",
    "lda1 = LinearDiscriminantAnalysis(solver=\"lsqr\",shrinkage=\"auto\")\n",
    "\n",
    "lda1.fit(X_train, y_train)\n",
    "  \n",
    "# Verificar las clases predichas\n",
    "y_pred1= lda1.predict(X_test_scaled)\n",
    "print(\"Clases predichas:\", np.unique(y_pred1))\n",
    "\n",
    "# Asegurarse de que haya más de una clase en y_test\n",
    "if len(np.unique(y_test)) < 2:\n",
    "    raise ValueError(\"y_test debe contener al menos dos clases para calcular ROC AUC.\")\n",
    "\n",
    "# Calcular el ROC AUC\n",
    "y_score1 = lda1.predict_proba(X_test_scaled)[:, 1]\n",
    "acc = roc_auc_score(y_test, y_score1)\n",
    "print(\"LDA - ROC AUC Score:\", acc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases predichas: [1]\n",
      "LDA - ROC AUC Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LDA 2\n",
    "lda2 = LinearDiscriminantAnalysis(solver=\"eigen\",shrinkage= 0.1, priors=[0.4, 0.6], store_covariance=False)\n",
    "\n",
    "lda2.fit(X_train, y_train)\n",
    "  \n",
    "# Verificar las clases predichas\n",
    "y_pred2= lda2.predict(X_test_scaled)\n",
    "print(\"Clases predichas:\", np.unique(y_pred2))\n",
    "\n",
    "# Asegurarse de que haya más de una clase en y_test\n",
    "if len(np.unique(y_test)) < 2:\n",
    "    raise ValueError(\"y_test debe contener al menos dos clases para calcular ROC AUC.\")\n",
    "\n",
    "# Calcular el ROC AUC\n",
    "y_score2 = lda2.predict_proba(X_test_scaled)[:, 1]\n",
    "acc = roc_auc_score(y_test, y_score2)\n",
    "print(\"LDA - ROC AUC Score:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases predichas: [1]\n",
      "LDA - ROC AUC Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LDA 3\n",
    "lda3 = LinearDiscriminantAnalysis(solver=\"eigen\",shrinkage= 0.1, priors=[0.4, 0.6], tol=1.0e-2)\n",
    "\n",
    "lda3.fit(X_train, y_train)\n",
    "  \n",
    "# Verificar las clases predichas\n",
    "y_pred3= lda3.predict(X_test_scaled)\n",
    "print(\"Clases predichas:\", np.unique(y_pred3))\n",
    "\n",
    "# Asegurarse de que haya más de una clase en y_test\n",
    "if len(np.unique(y_test)) < 2:\n",
    "    raise ValueError(\"y_test debe contener al menos dos clases para calcular ROC AUC.\")\n",
    "\n",
    "# Calcular el ROC AUC\n",
    "y_score3 = lda3.predict_proba(X_test_scaled)[:, 1]\n",
    "acc = roc_auc_score(y_test, y_score3)\n",
    "print(\"LDA - ROC AUC Score:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases predichas: [1]\n",
      "LDA - ROC AUC Score: 0.42246394771132517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LDA 4\n",
    "lda4 = LinearDiscriminantAnalysis(solver=\"eigen\", priors=[0.4, 0.6], tol=1.0e-9, store_covariance=True)\n",
    "\n",
    "lda4.fit(X_train, y_train)\n",
    "  \n",
    "# Verificar las clases predichas\n",
    "y_pred4= lda4.predict(X_test_scaled)\n",
    "print(\"Clases predichas:\", np.unique(y_pred4))\n",
    "\n",
    "# Asegurarse de que haya más de una clase en y_test\n",
    "if len(np.unique(y_test)) < 2:\n",
    "    raise ValueError(\"y_test debe contener al menos dos clases para calcular ROC AUC.\")\n",
    "\n",
    "# Calcular el ROC AUC\n",
    "y_score4 = lda4.predict_proba(X_test_scaled)[:, 1]\n",
    "acc = roc_auc_score(y_test, y_score4)\n",
    "print(\"LDA - ROC AUC Score:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases predichas: [1]\n",
      "LDA - ROC AUC Score: 0.4543791588195731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LDA 5\n",
    "lda5 = LinearDiscriminantAnalysis(solver=\"svd\", priors=[0.3, 0.7], tol=1.0e-9, store_covariance=False, n_components=1)\n",
    "\n",
    "lda5.fit(X_train, y_train)\n",
    "  \n",
    "# Verificar las clases predichas\n",
    "y_pred5= lda5.predict(X_test_scaled)\n",
    "print(\"Clases predichas:\", np.unique(y_pred5))\n",
    "\n",
    "# Asegurarse de que haya más de una clase en y_test\n",
    "if len(np.unique(y_test)) < 2:\n",
    "    raise ValueError(\"y_test debe contener al menos dos clases para calcular ROC AUC.\")\n",
    "\n",
    "# Calcular el ROC AUC\n",
    "y_score5 = lda5.predict_proba(X_test_scaled)[:, 1]\n",
    "acc = roc_auc_score(y_test, y_score5)\n",
    "print(\"LDA - ROC AUC Score:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'classifier__n_components': 1, 'classifier__shrinkage': 'auto', 'classifier__solver': 'lsqr'}\n",
      "Best score achieved:  0.9916\n",
      "LDA - ROC AUC Score: 0.4493090683491303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "95 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 621, in fit\n",
      "    raise NotImplementedError(\"shrinkage not supported with 'svd' solver.\")\n",
      "NotImplementedError: shrinkage not supported with 'svd' solver.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 614, in fit\n",
      "    raise ValueError(\n",
      "ValueError: n_components cannot be larger than min(n_features, n_classes - 1).\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.991      0.991      0.991             nan 0.9916     0.9916\n",
      "        nan 0.98876667 0.98876667        nan 0.96593333 0.96593333\n",
      "        nan 0.94643333 0.94643333        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LDA Best.fit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalado de datos\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Configuración de la búsqueda de hiperparámetros\n",
    "hyper_parameters = {\n",
    "    'classifier__solver': ['svd', 'lsqr', 'eigen'],\n",
    "    'classifier__shrinkage': [None, 'auto', 0.1, 0.5, 0.9] if 'lsqr' in ['lsqr', 'eigen'] or 'eigen' in ['lsqr', 'eigen'] else [None],\n",
    "    'classifier__n_components': [1, 2]\n",
    "}\n",
    "\n",
    "lda_hyp_opt = GridSearchCV(estimator=pipeline, param_grid=hyper_parameters, cv=5, scoring='accuracy')\n",
    "\n",
    "# Entrenar el modelo y buscar los mejores hiperparámetros\n",
    "lda_hyp_opt.fit(X, y)\n",
    "\n",
    "# Revisar los mejores hiperparámetros y el mejor modelo\n",
    "print(\"Best parameters found: \", lda_hyp_opt.best_params_)\n",
    "print(\"Best score achieved: \", lda_hyp_opt.best_score_)\n",
    "\n",
    "# Calcular el ROC AUC\n",
    "y_score6 = lda_hyp_opt.predict_proba(X_test_scaled)[:, 1]\n",
    "acc = roc_auc_score(y_test, y_score6)\n",
    "print(\"LDA - ROC AUC Score:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "logit.fit(X_train_scaled, y_train) #Entrenamiento del modelo\n",
    "\n",
    "logit_predicts = logit.predict(X_test_scaled)\n",
    " \n",
    "acc = roc_auc_score(logit_predicts,y_test)\n",
    "print(\"RL - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#logit 1\n",
    "logit1 = LogisticRegression(C=0.1, penalty='l2')\n",
    "logit1.fit(X_train_scaled, y_train) #Entrenamiento del modelo\n",
    "\n",
    "logit_predicts1 = logit1.predict(X_test_scaled)\n",
    " \n",
    "acc = roc_auc_score(logit_predicts1,y_test)\n",
    "print(\"RL - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL - Roc Auc Score:  0.9914976810546394\n"
     ]
    }
   ],
   "source": [
    "#logit 2\n",
    "logit2 = LogisticRegression(C=0.1,tol=1e-2, fit_intercept=True, intercept_scaling=1e-3)\n",
    "logit2.fit(X_train_scaled, y_train) #Entrenamiento del modelo\n",
    "\n",
    "logit_predicts2 = logit2.predict(X_test_scaled)\n",
    " \n",
    "acc = roc_auc_score(logit_predicts2,y_test)\n",
    "print(\"RL - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL - Roc Auc Score:  0.9915939821417599\n"
     ]
    }
   ],
   "source": [
    "#logit 3\n",
    "logit3 = LogisticRegression(C=0.1,tol=1e-2, fit_intercept=True, intercept_scaling=1e-3, solver=\"liblinear\")\n",
    "logit3.fit(X_train_scaled, y_train) #Entrenamiento del modelo\n",
    "\n",
    "logit_predicts3 = logit3.predict(X_test_scaled)\n",
    " \n",
    "acc = roc_auc_score(logit_predicts3,y_test)\n",
    "print(\"RL - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL - Roc Auc Score:  0.994683014354067\n"
     ]
    }
   ],
   "source": [
    "#logit 4\n",
    "logit4 = LogisticRegression(C=0.5,tol=1e-2, fit_intercept=True, intercept_scaling=1e-3, solver=\"liblinear\", class_weight=\"balanced\")\n",
    "logit4.fit(X_train_scaled, y_train) #Entrenamiento del modelo\n",
    "\n",
    "logit_predicts4 = logit4.predict(X_test_scaled)\n",
    " \n",
    "acc = roc_auc_score(logit_predicts4,y_test)\n",
    "print(\"RL - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL - Roc Auc Score:  0.994683014354067\n"
     ]
    }
   ],
   "source": [
    "#logit 5\n",
    "logit5 = LogisticRegression(C=0.5,tol=1e-2, fit_intercept=True, intercept_scaling=1e-3, solver=\"liblinear\", class_weight=\"balanced\", max_iter=200)\n",
    "logit5.fit(X_train_scaled, y_train) #Entrenamiento del modelo\n",
    "\n",
    "logit_predicts5 = logit5.predict(X_test_scaled)\n",
    " \n",
    "acc = roc_auc_score(logit_predicts5,y_test)\n",
    "print(\"RL - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'C': 0.1, 'class_weight': None, 'fit_intercept': True, 'max_iter': 100, 'solver': 'newton-cg'}\n",
      "Roc_Auc Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "hyper_parameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0],  # Valores de regularización\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],  # Métodos de optimización\n",
    "    'max_iter': [100, 200, 300],  # Número máximo de iteraciones\n",
    "    'class_weight': [None, 'balanced'],  # Opciones de peso de clase\n",
    "    'fit_intercept': [True, False]  # Si se debe ajustar la intercepción\n",
    "}\n",
    "\n",
    "# Crear el clasificador de Regresión Logística\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# Configurar la búsqueda de hiperparámetros con validación cruzada\n",
    "logit_grid_search = GridSearchCV(estimator=logit, param_grid=hyper_parameters, cv=5, scoring='roc_auc', error_score='raise')\n",
    "\n",
    "# Escalar los datos si es necesario (solo si se necesita estandarización)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "logit_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros:\", logit_grid_search.best_params_)\n",
    "\n",
    "# Predecir con el mejor modelo encontrado\n",
    "best_logit_classifier = logit_grid_search.best_estimator_\n",
    "logit_predicts = best_logit_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calcular el área bajo la curva ROC (ROC AUC) con los mejores hiperparámetros\n",
    "acc = roc_auc_score(y_test, logit_predicts)\n",
    "print(\"Roc_Auc Score:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9993457416976824\n"
     ]
    }
   ],
   "source": [
    "#svm  \n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_scaled,y_train)\n",
    "\n",
    "svm_predicts = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(svm_predicts,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9998855835240275\n"
     ]
    }
   ],
   "source": [
    "#svm 1\n",
    "svm_classifier1 = SVC(kernel='poly', C=0.5, random_state=42, gamma=\"scale\", degree=5, coef0=0.3, shrinking=True)\n",
    "svm_classifier1.fit(X_train_scaled,y_train)\n",
    "\n",
    "svm_predicts1 = svm_classifier1.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(svm_predicts1,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9993457416976824\n"
     ]
    }
   ],
   "source": [
    "#svm 2\n",
    "svm_classifier2 = SVC(kernel='rbf', C=0.8, gamma=\"scale\", degree=8, coef0=0.5, shrinking=True)\n",
    "svm_classifier2.fit(X_train_scaled,y_train)\n",
    "\n",
    "svm_predicts2 = svm_classifier2.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(svm_predicts2,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9993457416976824\n"
     ]
    }
   ],
   "source": [
    "#svm 3\n",
    "svm_classifier3 = SVC(kernel='rbf', C=0.8, gamma=\"scale\", degree=8, coef0=0.5, shrinking=True, probability=True)\n",
    "svm_classifier3.fit(X_train_scaled,y_train)\n",
    "\n",
    "svm_predicts3 = svm_classifier3.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(svm_predicts3,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9993457416976824\n"
     ]
    }
   ],
   "source": [
    "#svm 4\n",
    "svm_classifier4 = SVC(kernel='rbf', C=0.8, gamma=\"scale\", cache_size=500, degree=8, coef0=0.5, shrinking=True, probability=True)\n",
    "svm_classifier4.fit(X_train_scaled,y_train)\n",
    "\n",
    "svm_predicts4 = svm_classifier4.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(svm_predicts4,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]NB - Roc Auc Score:  0.9993457416976824\n"
     ]
    }
   ],
   "source": [
    "#svm 5\n",
    "svm_classifier5 = SVC(kernel='rbf', C=0.8, gamma=\"scale\", cache_size=500, degree=8, coef0=0.5, shrinking=True, probability=True, verbose=5)\n",
    "svm_classifier5.fit(X_train_scaled,y_train)\n",
    "\n",
    "svm_predicts5 = svm_classifier5.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(svm_predicts5,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9993457416976824\n"
     ]
    }
   ],
   "source": [
    "#svm 6\n",
    "svm_classifier6 = SVC(C=1.0,\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    degree=3,\n",
    "    coef0=0.0,\n",
    "    shrinking=True,\n",
    "    probability=False,\n",
    "    tol=1e-3,\n",
    "    cache_size=200,\n",
    "    class_weight=None,\n",
    "    verbose=False,\n",
    "    max_iter=-1)\n",
    "svm_classifier6.fit(X_train_scaled,y_train)\n",
    "\n",
    "svm_predicts6 = svm_classifier6.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(svm_predicts6,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Arboles de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#DTC\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(X_train_scaled,y_train)\n",
    "\n",
    "dt_predicts = dt_classifier.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(dt_predicts,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#DTC 1\n",
    "dt_classifier1 = DecisionTreeClassifier(criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=42,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0)\n",
    "dt_classifier1.fit(X_train_scaled,y_train)\n",
    "\n",
    "dt_predicts1 = dt_classifier1.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(dt_predicts1,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#DTC 2\n",
    "dt_classifier2 = DecisionTreeClassifier(criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2, \n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None, \n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0, \n",
    "    ccp_alpha=0.0)\n",
    "dt_classifier2.fit(X_train_scaled,y_train)\n",
    "\n",
    "dt_predicts2 = dt_classifier2.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(dt_predicts2,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#DTC 3\n",
    "dt_classifier3 = DecisionTreeClassifier(criterion='log_loss',\n",
    "    splitter='best', \n",
    "    min_samples_leaf=1, \n",
    "    max_features=None,\n",
    "    random_state=42,\n",
    "    max_leaf_nodes=None, \n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0)\n",
    "dt_classifier3.fit(X_train_scaled,y_train)\n",
    "\n",
    "dt_predicts3 = dt_classifier3.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(dt_predicts3,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#DTC 4\n",
    "dt_classifier4 = DecisionTreeClassifier(criterion='entropy',\n",
    "    splitter='random',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=42,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0)\n",
    "dt_classifier4.fit(X_train_scaled,y_train)\n",
    "\n",
    "dt_predicts4 = dt_classifier4.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(dt_predicts4,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB - Roc Auc Score:  0.9530863712195213\n"
     ]
    }
   ],
   "source": [
    "#DTC 5\n",
    "dt_classifier5 = DecisionTreeClassifier(criterion='entropy',\n",
    "    splitter='random',\n",
    "    max_depth=None,\n",
    "    min_samples_split=0.5,\n",
    "    min_samples_leaf=5,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,     \n",
    "    random_state=42,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0)\n",
    "dt_classifier5.fit(X_train_scaled,y_train)\n",
    "\n",
    "dt_predicts5 = dt_classifier5.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(dt_predicts5,y_test)\n",
    "print(\"NB - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train_scaled,y_train)\n",
    "\n",
    "rf_predicts = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(rf_predicts,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#RF 1\n",
    "rf_classifier1 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='sqrt',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0\n",
    "    )\n",
    "rf_classifier1.fit(X_train_scaled,y_train)\n",
    "\n",
    "rf_predicts1 = rf_classifier1.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(rf_predicts1,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#RF 2\n",
    "rf_classifier2 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy', #'gini', 'entropy', 'log_loss'\n",
    "    max_depth=50,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=20, #int, float, 'auto', 'sqrt', 'log2', None\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None, # dict, 'balanced', None\n",
    "    ccp_alpha=0.0\n",
    "    )\n",
    "\n",
    "rf_classifier2.fit(X_train_scaled,y_train)\n",
    "\n",
    "rf_predicts2 = rf_classifier2.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(rf_predicts2,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#RF 3\n",
    "rf_classifier3 = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    criterion='log_loss', #'gini', 'entropy', 'log_loss'\n",
    "    max_depth=50,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='log2', #int, float, 'auto', 'sqrt', 'log2', None\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight='balanced', # dict, 'balanced', None\n",
    "    ccp_alpha=0.0\n",
    "    )\n",
    "\n",
    "rf_classifier3.fit(X_train_scaled,y_train)\n",
    "\n",
    "rf_predicts3 = rf_classifier3.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(rf_predicts3,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#RF 4\n",
    "rf_classifier4= RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    criterion='gini', #'gini', 'entropy', 'log_loss'\n",
    "    max_depth=50, \n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='sqrt', #int, float, 'auto', 'sqrt', 'log2', None\n",
    "    max_leaf_nodes=None, \n",
    "    bootstrap=False,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42, \n",
    "    warm_start=False,\n",
    "    class_weight='balanced', # dict, 'balanced', None\n",
    "    ccp_alpha=0.0\n",
    "    )\n",
    "\n",
    "rf_classifier4.fit(X_train_scaled,y_train)\n",
    "\n",
    "rf_predicts4 = rf_classifier4.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(rf_predicts4,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#RF 5\n",
    "rf_classifier5 = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    criterion='gini', #'gini', 'entropy', 'log_loss'\n",
    "    max_depth=100, \n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='sqrt', #int, float, 'auto', 'sqrt', 'log2', None \n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight='balanced', # dict, 'balanced', None\n",
    "    ccp_alpha=0.0\n",
    "    )\n",
    "\n",
    "rf_classifier5.fit(X_train_scaled,y_train)\n",
    "\n",
    "rf_predicts5 = rf_classifier5.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(rf_predicts5,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 QDA – Análisis de Discriminante Cuadrático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  0.9997056362485792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "#QDA\n",
    "qda_classifier  = QuadraticDiscriminantAnalysis()\n",
    "qda_classifier .fit(X_train_scaled,y_train)\n",
    "\n",
    "qda_predicts = qda_classifier.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(qda_predicts,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  0.9997056362485792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "#QDA 1\n",
    "qda_classifier1  = QuadraticDiscriminantAnalysis(\n",
    "    priors=None,            # No especificar probabilidades a priori\n",
    "    reg_param=0.0,          # Sin regularización\n",
    "    store_covariance=False, # No almacenar matrices de covarianza\n",
    "    tol=1e-4                # Tolerancia para descartar componentes\n",
    ")\n",
    "qda_classifier1 .fit(X_train_scaled,y_train)\n",
    "\n",
    "qda_predicts1 = qda_classifier1.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(qda_predicts1,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  0.9652971195759629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "#QDA 2\n",
    "qda_classifier2  = QuadraticDiscriminantAnalysis(\n",
    "    priors=[0.4,0.6],            # No especificar probabilidades a priori\n",
    "    reg_param=0.5,          # Sin regularización\n",
    "    store_covariance=False, # No almacenar matrices de covarianza\n",
    "    tol=1e-4                # Tolerancia para descartar componentes\n",
    ")\n",
    "qda_classifier2 .fit(X_train_scaled,y_train)\n",
    "\n",
    "qda_predicts2 = qda_classifier2.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(qda_predicts2,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  0.9204115239145367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "#QDA 3\n",
    "qda_classifier3  = QuadraticDiscriminantAnalysis(\n",
    "    priors=[0.3,0.7],            # No especificar probabilidades a priori\n",
    "    reg_param=0.9,          # Sin regularización\n",
    "    store_covariance=True, # No almacenar matrices de covarianza\n",
    "    tol=1e-9               # Tolerancia para descartar componentes\n",
    ")\n",
    "qda_classifier3 .fit(X_train_scaled,y_train)\n",
    "\n",
    "qda_predicts3 = qda_classifier3.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(qda_predicts3,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  0.9059595568595755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "#QDA 4\n",
    "qda_classifier4  = QuadraticDiscriminantAnalysis(\n",
    "    priors=[0.3,0.7],            # No especificar probabilidades a priori\n",
    "    reg_param=1.0,          # Sin regularización\n",
    "    store_covariance=True, # No almacenar matrices de covarianza\n",
    "    tol=1e-10             # Tolerancia para descartar componentes\n",
    ")\n",
    "qda_classifier4 .fit(X_train_scaled,y_train)\n",
    "\n",
    "qda_predicts4 = qda_classifier4.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(qda_predicts4,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Crear un clasificador AdaBoost\n",
    "adaboost_classifier = AdaBoostClassifier()\n",
    "adaboost_classifier .fit(X_train_scaled,y_train)\n",
    "\n",
    "adaboost_predicts = adaboost_classifier.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(adaboost_predicts,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# AB 1\n",
    "adaboost_classifier1 = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',\n",
    "    random_state=2024\n",
    ")\n",
    "adaboost_classifier1 .fit(X_train_scaled,y_train)\n",
    "\n",
    "adaboost_predicts1 = adaboost_classifier1.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(adaboost_predicts1,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# AB 2\n",
    "adaboost_classifier2 = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=200,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',\n",
    "    random_state=2024\n",
    ")\n",
    "adaboost_classifier2 .fit(X_train_scaled,y_train)\n",
    "\n",
    "adaboost_predicts2 = adaboost_classifier2.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(adaboost_predicts2,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jggl_\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# AB 3\n",
    "adaboost_classifier3 = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.8,\n",
    "    algorithm='SAMME.R',\n",
    "    random_state=2024\n",
    ")\n",
    "adaboost_classifier3 .fit(X_train_scaled,y_train)\n",
    "\n",
    "adaboost_predicts3 = adaboost_classifier3.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(adaboost_predicts3,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# GB\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "gb_classifier .fit(X_train_scaled,y_train)\n",
    "\n",
    "gb_predicts = gb_classifier.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(gb_predicts,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1149            1.30s\n",
      "         2           0.9644            1.53s\n",
      "         3           0.8412            1.22s\n",
      "         4           0.7383            1.29s\n",
      "         5           0.6510            1.65s\n",
      "         6           0.5761            1.80s\n",
      "         7           0.5114            1.90s\n",
      "         8           0.4551            1.82s\n",
      "         9           0.4059            1.75s\n",
      "        10           0.3626            1.79s\n",
      "        20           0.1243            1.55s\n",
      "        30           0.0447            1.26s\n",
      "        40           0.0163            0.97s\n",
      "        50           0.0060            0.76s\n",
      "        60           0.0022            0.58s\n",
      "        70           0.0008            0.42s\n",
      "        80           0.0003            0.27s\n",
      "        90           0.0001            0.13s\n",
      "       100           0.0000            0.00s\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# GB 1\n",
    "gb_classifier1 = GradientBoostingClassifier(\n",
    "    loss='log_loss',        # Función de pérdida\n",
    "    learning_rate=0.1,      # Tasa de aprendizaje\n",
    "    n_estimators=100,       # Número de estimadores\n",
    "    subsample=1.0,          # Submuestreo\n",
    "    criterion='friedman_mse',# Criterio de calidad\n",
    "    min_samples_split=2,    # Mínimas muestras para dividir un nodo\n",
    "    min_samples_leaf=1,     # Mínimas muestras en un nodo hoja\n",
    "    max_depth=3,            # Profundidad máxima\n",
    "    max_features=None,      # Todas las características\n",
    "    random_state=42,        # Estado aleatorio para reproducibilidad\n",
    "    verbose=1               # Nivel de verbosidad\n",
    ")\n",
    "gb_classifier1 .fit(X_train_scaled,y_train)\n",
    "\n",
    "gb_predicts1 = gb_classifier1.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(gb_predicts1,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.8691           0.0907           37.18s\n",
      "         2           0.7837           0.0713           27.39s\n",
      "         3           0.7102           0.0796           30.84s\n",
      "         4           0.6431           0.0694           28.28s\n",
      "         5           0.5813           0.0586           25.38s\n",
      "         6           0.5275           0.0618           21.13s\n",
      "         7           0.4758           0.0435           20.31s\n",
      "         8           0.4307           0.0463           23.63s\n",
      "         9           0.3900           0.0421           23.27s\n",
      "        10           0.3528           0.0369           22.48s\n",
      "        20           0.1297           0.0137           18.83s\n",
      "        30           0.0477           0.0053           19.71s\n",
      "        40           0.0176           0.0018           18.41s\n",
      "        50           0.0065           0.0006           16.54s\n",
      "        60           0.0024           0.0003           15.12s\n",
      "        70           0.0009           0.0001           14.07s\n",
      "        80           0.0003           0.0000           13.31s\n",
      "        90           0.0001           0.0000           12.72s\n",
      "       100           0.0000           0.0000           12.16s\n",
      "       200           0.0000          -0.0000            8.67s\n",
      "       300           0.0000          -0.0000            5.58s\n",
      "       400           0.0000           0.0000            3.91s\n",
      "       500           0.0000           0.0000            2.79s\n",
      "       600           0.0000          -0.0000            1.99s\n",
      "       700           0.0000          -0.0000            1.36s\n",
      "       800           0.0000          -0.0000            0.84s\n",
      "       900           0.0000          -0.0000            0.39s\n",
      "      1000           0.0000           0.0000            0.00s\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# GB 2\n",
    "gb_classifier2 = GradientBoostingClassifier(\n",
    "    loss='exponential',        # Función de pérdida\n",
    "    learning_rate=0.1,      # Tasa de aprendizaje\n",
    "    n_estimators=1000,       # Número de estimadores\n",
    "    subsample=0.8,          # Submuestreo\n",
    "    criterion='squared_error',# Criterio de calidad\n",
    "    min_samples_split=2,    # Mínimas muestras para dividir un nodo\n",
    "    min_samples_leaf=1,     # Mínimas muestras en un nodo hoja\n",
    "    max_depth=5,            # Profundidad máxima\n",
    "    max_features=None,      # Todas las características\n",
    "    random_state=42,        # Estado aleatorio para reproducibilidad\n",
    "    verbose=1               # Nivel de verbosidad\n",
    ")\n",
    "gb_classifier2 .fit(X_train_scaled,y_train)\n",
    "\n",
    "gb_predicts2 = gb_classifier2.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(gb_predicts2,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.7970           0.5072            0.00s\n",
      "         2           0.5376           0.2566            6.16s\n",
      "         3           0.3765           0.1618            5.55s\n",
      "         4           0.2681           0.1072            5.92s\n",
      "         5           0.1935           0.0749            7.21s\n",
      "         6           0.1415           0.0541            6.76s\n",
      "         7           0.1031           0.0373            8.25s\n",
      "         8           0.0757           0.0275            7.64s\n",
      "         9           0.0556           0.0199            7.25s\n",
      "        10           0.0410           0.0146            7.53s\n",
      "        20           0.0020           0.0007            5.38s\n",
      "        30           0.0001           0.0000            4.99s\n",
      "        40           0.0000           0.0000            4.56s\n",
      "        50           0.0000           0.0000            4.29s\n",
      "        60           0.0000           0.0000            3.92s\n",
      "        70           0.0000           0.0000            3.52s\n",
      "        80           0.0000           0.0000            3.10s\n",
      "        90           0.0000          -0.0000            2.76s\n",
      "       100           0.0000           0.0000            2.55s\n",
      "       200           0.0000           0.0000            1.29s\n",
      "       300           0.0000          -0.0000            0.72s\n",
      "       400           0.0000           0.0000            0.33s\n",
      "       500           0.0000          -0.0000            0.00s\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# GB 3\n",
    "gb_classifier3 = GradientBoostingClassifier(\n",
    "    loss='log_loss',        # Función de pérdida\n",
    "    learning_rate=0.3,      # Tasa de aprendizaje\n",
    "    n_estimators=500,       # Número de estimadores\n",
    "    subsample=0.6,          # Submuestreo\n",
    "    criterion='squared_error',# Criterio de calidad \n",
    "    max_depth=5,            # Profundidad máxima\n",
    "    max_features=None,      # Todas las características\n",
    "    random_state=42,        # Estado aleatorio para reproducibilidad\n",
    "    verbose=1               # Nivel de verbosidad\n",
    ")\n",
    "gb_classifier3 .fit(X_train_scaled,y_train)\n",
    "\n",
    "gb_predicts3 = gb_classifier3.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(gb_predicts3,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.4762            1.03s\n",
      "         2           0.2365            8.73s\n",
      "         3           0.1174            7.70s\n",
      "         4           0.0583            7.45s\n",
      "         5           0.0290            5.95s\n",
      "         6           0.0144            6.24s\n",
      "         7           0.0071            6.44s\n",
      "         8           0.0035            6.28s\n",
      "         9           0.0018            6.32s\n",
      "        10           0.0009            6.02s\n",
      "        20           0.0000            5.50s\n",
      "        30           0.0000            4.57s\n",
      "        40           0.0000            3.53s\n",
      "        50           0.0000            2.90s\n",
      "        60           0.0000            2.48s\n",
      "        70           0.0000            2.08s\n",
      "        80           0.0000            1.86s\n",
      "        90           0.0000            1.69s\n",
      "       100           0.0000            1.54s\n",
      "       200           0.0000            0.77s\n",
      "       300           0.0000            0.42s\n",
      "       400           0.0000            0.19s\n",
      "       500           0.0000            0.00s\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# GB 4\n",
    "gb_classifier4 = GradientBoostingClassifier(\n",
    "    loss='exponential',        # Función de pérdida\n",
    "    learning_rate=0.7,      # Tasa de aprendizaje\n",
    "    n_estimators=500,       # Número de estimadores\n",
    "    subsample=1,          # Submuestreo\n",
    "    criterion='friedman_mse',# Criterio de calidad \n",
    "    max_depth=1,            # Profundidad máxima\n",
    "    max_features=None,      # Todas las características\n",
    "    random_state=42,        # Estado aleatorio para reproducibilidad\n",
    "    verbose=1               # Nivel de verbosidad\n",
    ")\n",
    "gb_classifier4.fit(X_train_scaled,y_train)\n",
    "\n",
    "gb_predicts4 = gb_classifier4.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(gb_predicts4,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8677           15.42s\n",
      "         2           0.7851           12.89s\n",
      "         3           0.7104           12.06s\n",
      "         4           0.6428           10.76s\n",
      "         5           0.5816           11.73s\n",
      "         6           0.5263            9.76s\n",
      "         7           0.4762            9.45s\n",
      "         8           0.4309           10.17s\n",
      "         9           0.3899           10.73s\n",
      "        10           0.3528           10.40s\n",
      "        20           0.1298            8.11s\n",
      "        30           0.0477            8.46s\n",
      "        40           0.0176            7.71s\n",
      "        50           0.0065            7.02s\n",
      "        60           0.0024            6.41s\n",
      "        70           0.0009            6.05s\n",
      "        80           0.0003            5.75s\n",
      "        90           0.0001            5.49s\n",
      "       100           0.0000            5.23s\n",
      "       200           0.0000            3.31s\n",
      "       300           0.0000            1.55s\n",
      "       400           0.0000            0.61s\n",
      "       500           0.0000            0.00s\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# GB 5\n",
    "gb_classifier5 = GradientBoostingClassifier(\n",
    "    loss='exponential',        # Función de pérdida \n",
    "    n_estimators=500,       # Número de estimadores\n",
    "    subsample=1,          # Submuestreo  \n",
    "    random_state=42,        # Estado aleatorio para reproducibilidad\n",
    "    verbose=1               # Nivel de verbosidad\n",
    ")\n",
    "gb_classifier5.fit(X_train_scaled,y_train)\n",
    "\n",
    "gb_predicts5 = gb_classifier5.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(gb_predicts5,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# XGB \n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier .fit(X_train_scaled,y_train)\n",
    "\n",
    "xgb_predicts = xgb_classifier.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(xgb_predicts,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# XGB 1\n",
    "xgb_classifier1 = xgb.XGBClassifier(\n",
    "    n_estimators=100,           # Número de árboles\n",
    "    learning_rate=0.1,          # Tasa de aprendizaje\n",
    "    max_depth=6,                # Profundidad máxima de los árboles\n",
    "    min_child_weight=1,         # Peso mínimo en un nodo hijo\n",
    "    subsample=0.8,              # Fracción de muestras para entrenar cada árbol\n",
    "    colsample_bytree=0.8,       # Fracción de características para entrenar cada árbol\n",
    "    gamma=0,                    # Mínima reducción de la función de pérdida para dividir un nodo\n",
    "    reg_alpha=0,                # Regularización L1\n",
    "    reg_lambda=1,               # Regularización L2\n",
    "    scale_pos_weight=1,         # Balance de clases\n",
    "    objective='binary:logistic', # Función de objetivo para clasificación multiclase\n",
    "    booster='gbtree',           # Tipo de modelo base\n",
    "    n_jobs=-1,                  # Usar todos los núcleos disponibles\n",
    "    random_state=42,            # Estado aleatorio para reproducibilidad\n",
    "    verbosity=1                 # Nivel de verbosidad\n",
    ")\n",
    "\n",
    "\n",
    "xgb_classifier1 .fit(X_train_scaled,y_train)\n",
    "\n",
    "xgb_predicts1 = xgb_classifier1.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(xgb_predicts1,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# XGB 2\n",
    "xgb_classifier2 = xgb.XGBClassifier(\n",
    "    n_estimators=200,           # Número de árboles\n",
    "    learning_rate=0.1,          # Tasa de aprendizaje\n",
    "    max_depth=6,                # Profundidad máxima de los árboles\n",
    "    min_child_weight=1,         # Peso mínimo en un nodo hijo\n",
    "    subsample=0.8,              # Fracción de muestras para entrenar cada árbol\n",
    "    colsample_bytree=0.8,       # Fracción de características para entrenar cada árbol\n",
    "    gamma=0,                    # Mínima reducción de la función de pérdida para dividir un nodo\n",
    "    reg_alpha=0,                # Regularización L1\n",
    "    reg_lambda=1,               # Regularización L2\n",
    "    scale_pos_weight=1,         # Balance de clases\n",
    "    objective='binary:logistic', # Función de objetivo para clasificación multiclase {'binary:logistic', 'multi:softmax', 'multi:softprob', etc.}\n",
    "    booster='gbtree',           # Tipo de modelo base {'gbtree', 'gblinear', 'dart'}\n",
    "    n_jobs=-1,                  # Usar todos los núcleos disponibles\n",
    "    random_state=2024,            # Estado aleatorio para reproducibilidad\n",
    "    verbosity=2                 # Nivel de verbosidad\n",
    ")\n",
    "\n",
    "\n",
    "xgb_classifier2 .fit(X_train_scaled,y_train)\n",
    "\n",
    "xgb_predicts2 = xgb_classifier2.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(xgb_predicts2,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# XGB 3\n",
    "xgb_classifier3 = xgb.XGBClassifier(\n",
    "    n_estimators=200,           # Número de árboles\n",
    "    learning_rate=0.1,          # Tasa de aprendizaje\n",
    "    max_depth=6,                # Profundidad máxima de los árboles\n",
    "    min_child_weight=1,         # Peso mínimo en un nodo hijo\n",
    "    subsample=0.8,              # Fracción de muestras para entrenar cada árbol\n",
    "    colsample_bytree=0.8,       # Fracción de características para entrenar cada árbol\n",
    "    gamma=0,                    # Mínima reducción de la función de pérdida para dividir un nodo\n",
    "    reg_alpha=0,                # Regularización L1\n",
    "    reg_lambda=1,               # Regularización L2\n",
    "    scale_pos_weight=1,         # Balance de clases\n",
    "    objective='binary:logistic', # Función de objetivo para clasificación multiclase {'binary:logistic', 'multi:softmax', 'multi:softprob', etc.}\n",
    "    booster='gbtree',           # Tipo de modelo base {'gbtree', 'gblinear', 'dart'}\n",
    "    n_jobs=-1,                  # Usar todos los núcleos disponibles\n",
    "    random_state=2024,            # Estado aleatorio para reproducibilidad\n",
    "    verbosity=2                 # Nivel de verbosidad\n",
    ")\n",
    "\n",
    "\n",
    "xgb_classifier3 .fit(X_train_scaled,y_train)\n",
    "\n",
    "xgb_predicts3 = xgb_classifier3.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(xgb_predicts3,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# XGB 4\n",
    "xgb_classifier4 = xgb.XGBClassifier(\n",
    "    n_estimators=200,           # Número de árboles\n",
    "    learning_rate=0.1,          # Tasa de aprendizaje    \n",
    "    reg_alpha=0,                # Regularización L1\n",
    "    reg_lambda=1,               # Regularización L2\n",
    "    scale_pos_weight=1,         # Balance de clases\n",
    "    objective='binary:logistic', # Función de objetivo para clasificación multiclase {'binary:logistic', 'multi:softmax', 'multi:softprob', etc.}\n",
    "    booster='gbtree',           # Tipo de modelo base {'gbtree', 'gblinear', 'dart'}\n",
    "    n_jobs=-1,                  # Usar todos los núcleos disponibles\n",
    "    random_state=2024,            # Estado aleatorio para reproducibilidad\n",
    "    verbosity=1                 # Nivel de verbosidad\n",
    ")\n",
    "\n",
    "\n",
    "xgb_classifier4.fit(X_train_scaled,y_train)\n",
    "\n",
    "xgb_predicts4 = xgb_classifier4.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(xgb_predicts4,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# XGB 5\n",
    "xgb_classifier5 = xgb.XGBClassifier(\n",
    "    n_estimators=200,           # Número de árboles\n",
    "    learning_rate=0.1,          # Tasa de aprendizaje    \n",
    "    scale_pos_weight=3,         # Balance de clases\n",
    "    objective='binary:logistic', # Función de objetivo para clasificación multiclase {'binary:logistic', 'multi:softmax', 'multi:softprob', etc.}\n",
    "    booster='dart',           # Tipo de modelo base {'gbtree', 'gblinear', 'dart'}\n",
    "    n_jobs=-1,                  # Usar todos los núcleos disponibles\n",
    "    random_state=2024,            # Estado aleatorio para reproducibilidad\n",
    "    verbosity=1                 # Nivel de verbosidad\n",
    ")\n",
    "\n",
    "\n",
    "xgb_classifier5.fit(X_train_scaled,y_train)\n",
    "\n",
    "xgb_predicts5 = xgb_classifier5.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(xgb_predicts5,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6448, number of negative: 11552\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 18000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.358222 -> initscore=-0.583089\n",
      "[LightGBM] [Info] Start training from score -0.583089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "# Crear un clasificador LightGBM\n",
    "lgb_classifier = lgb.LGBMClassifier( )\n",
    "\n",
    "lgb_classifier .fit(X_train_scaled,y_train)\n",
    "\n",
    "lgb_predicts = lgb_classifier.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(lgb_predicts,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6448, number of negative: 11552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 18000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.358222 -> initscore=-0.583089\n",
      "[LightGBM] [Info] Start training from score -0.583089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#  LightGBM1\n",
    "lgb_classifier1 = lgb.LGBMClassifier( \n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    num_leaves=20,\n",
    "    max_depth=10,\n",
    "    min_child_samples=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=0    \n",
    "    )\n",
    "\n",
    "lgb_classifier1 .fit(X_train_scaled,y_train)\n",
    "\n",
    "lgb_predicts1 = lgb_classifier1.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(lgb_predicts1,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6448, number of negative: 11552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 18000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.358222 -> initscore=-0.583089\n",
      "[LightGBM] [Info] Start training from score -0.583089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#  LightGBM2\n",
    "lgb_classifier2 = lgb.LGBMClassifier( \n",
    "    n_estimators=50, \n",
    "    learning_rate=0.01, \n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1    \n",
    "    )\n",
    "\n",
    "lgb_classifier2 .fit(X_train_scaled,y_train)\n",
    "\n",
    "lgb_predicts2 = lgb_classifier2.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(lgb_predicts2,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6448, number of negative: 11552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 18000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.358222 -> initscore=-0.583089\n",
      "[LightGBM] [Info] Start training from score -0.583089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#  LightGBM3\n",
    "lgb_classifier3 = lgb.LGBMClassifier( \n",
    "    n_estimators=200, \n",
    "    learning_rate=0.2, \n",
    "    num_leaves=40,\n",
    "    max_depth=-20,\n",
    "    min_child_samples=30,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1    \n",
    "    )\n",
    "\n",
    "lgb_classifier3 .fit(X_train_scaled,y_train)\n",
    "\n",
    "lgb_predicts3 = lgb_classifier3.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(lgb_predicts3,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6448, number of negative: 11552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 18000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.358222 -> initscore=-0.583089\n",
      "[LightGBM] [Info] Start training from score -0.583089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#  LightGBM4\n",
    "lgb_classifier4 = lgb.LGBMClassifier( \n",
    "    n_estimators=200, \n",
    "    learning_rate=0.2,  \n",
    "    min_child_samples=30,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0, \n",
    "    reg_lambda=1    \n",
    "    )\n",
    "\n",
    "lgb_classifier4 .fit(X_train_scaled,y_train)\n",
    "\n",
    "lgb_predicts4 = lgb_classifier4.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(lgb_predicts4,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6448, number of negative: 11552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 18000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.358222 -> initscore=-0.583089\n",
      "[LightGBM] [Info] Start training from score -0.583089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomForest - Roc Auc Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#  LightGBM5\n",
    "lgb_classifier5 = lgb.LGBMClassifier( \n",
    "    n_estimators=100, \n",
    "    min_child_samples=30,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1    \n",
    "    )\n",
    "\n",
    "lgb_classifier5 .fit(X_train_scaled,y_train)\n",
    "\n",
    "lgb_predicts5 = lgb_classifier5.predict(X_test_scaled)\n",
    "\n",
    "acc = roc_auc_score(lgb_predicts5,y_test)\n",
    "print(\"RandomForest - Roc Auc Score: \",acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
